{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+nXl7ZKHmjcN1+dNEBMqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vansh-virmani/NextGenAI/blob/main/NLP0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UlCDU795x2J",
        "outputId": "3cd5410e-3b52-4972-b37d-79c2e4bd97e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/nlp\")"
      ],
      "metadata": {
        "id": "pk2sst_6-Qmi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"You currently have zero computing units available.\"\n",
        "text=text.lower()\n",
        "print(\"lower text: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OD5uNmm69Eq",
        "outputId": "1d9b401d-571f-4cda-bdd9-7a333e1039d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower text:  you currently have zero computing units available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lower the text"
      ],
      "metadata": {
        "id": "8xD1KMBT8J5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step2 Remove Punctuation\n",
        "\n",
        "#we remove everything except letters and spaces\n",
        "# Using re-regular expression\n",
        "import re\n",
        "text=re.sub(r'[^\\w\\s]','',text)\n",
        "print(\"Without puncttuation: \",text)\n",
        "#[^\\w\\s]  to remove evrything except letters and spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTGuW4Li8UJ2",
        "outputId": "ed35716c-7ac0-4dd0-954c-a5c196ed7d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without puncttuation:  you currently have zero computing units available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3 ***Tokenization***"
      ],
      "metadata": {
        "id": "cvNGunej9B54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split sentence into tokens\n",
        "tokens=text.split()\n",
        "print(\"tokens\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie6zltdB9A4K",
        "outputId": "2de2fe0e-3996-4bf4-dab1-5bfea56b07f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens ['you', 'currently', 'have', 'zero', 'computing', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step4 remove stopwords- is am are the a an very\n",
        "stopwords=[\"i\",\"am\",\"are\",\"have\",\"a\",\"an\",\"very\"]\n",
        "filtered_tokens=[]\n",
        "for word in tokens:\n",
        "  if word not in stopwords:\n",
        "    filtered_tokens.append(word)\n"
      ],
      "metadata": {
        "id": "52Ce19Bn9j4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"filtered tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ueEthw-bWn",
        "outputId": "4ae69bbf-0aeb-43b9-fa8d-67c2895ee91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filtered tokens: ['you', 'currently', 'zero', 'computing', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(word):\n",
        "  if word.endswith(\"ing\"):\n",
        "    return word[:-3]\n",
        "  return word\n",
        "stemmed_words=[]\n",
        "for word in filtered_tokens:\n",
        "  stemmed_words.append(stem(word))\n",
        "\n",
        "print(\"After stemming: \", stemmed_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sCNsE1e-mzC",
        "outputId": "c40202bc-b8f2-49db-c951-8f55814d8ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming:  ['you', 'currently', 'zero', 'comput', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP phase 2 word to vector\n",
        "#why ml- because ml only understands numbers text->numbrs vectorz\n",
        "from sklearn.feature_extraction.text import CountVectorizer #converting text to count\n",
        "sentences = [\n",
        "    \"I am learning Python every day\",\n",
        "    \"She likes to read books in the evening\",\n",
        "    \"We are building a small project together\",\n",
        "    \"He wants to become a data scientist\",\n",
        "    \"They are playing cricket in the park\",\n",
        "    \"I enjoy watching movies on weekends\",\n",
        "    \"The weather is very pleasant today\",\n",
        "    \"She is preparing for her exams seriously\",\n",
        "    \"We will start the project next week\",\n",
        "    \"He practices coding for two hours daily\",\n",
        "    \"The cat is sleeping on the sofa\",\n",
        "    \"I am trying to improve my communication skills\",\n",
        "    \"They are planning a trip to the mountains\",\n",
        "    \"She loves listening to music while studying\",\n",
        "    \"We are learning new technologies in college\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "JvJdBFgfAPQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)# learn the vocab and convert to numbers(count)\n",
        "print(\"Vocabluary :\", vectorizer.get_feature_names_out())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3XkN8FRBDyq",
        "outputId": "508ed4e6-ecba-4d68-9ca2-23935d0bf265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabluary : ['am' 'are' 'become' 'books' 'building' 'cat' 'coding' 'college'\n",
            " 'communication' 'cricket' 'daily' 'data' 'day' 'enjoy' 'evening' 'every'\n",
            " 'exams' 'for' 'he' 'her' 'hours' 'improve' 'in' 'is' 'learning' 'likes'\n",
            " 'listening' 'loves' 'mountains' 'movies' 'music' 'my' 'new' 'next' 'on'\n",
            " 'park' 'planning' 'playing' 'pleasant' 'practices' 'preparing' 'project'\n",
            " 'python' 'read' 'scientist' 'seriously' 'she' 'skills' 'sleeping' 'small'\n",
            " 'sofa' 'start' 'studying' 'technologies' 'the' 'they' 'to' 'today'\n",
            " 'together' 'trip' 'trying' 'two' 'very' 'wants' 'watching' 'we' 'weather'\n",
            " 'week' 'weekends' 'while' 'will']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors: \")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u53CsxrFGf46",
        "outputId": "09a1e586-a579-43fa-d809-cc8bdb9ec5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors: \n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-2\n",
        "# here each row= sentence\n",
        "# each column=word\n",
        "# each value=count\n",
        "sentences = [\n",
        "    \"I love coding\",\n",
        "    \"She reads books\",\n",
        "    \"We learn Python\",\n",
        "    \"He plays cricket\",\n",
        "    \"They study daily\"\n",
        "]\n",
        "vectorizer=CountVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)# learn the vocab and convert to numbers\n",
        "print(\"Vocabluary :\", vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrWoUy8HGpwP",
        "outputId": "3d488082-cbe3-4e93-bd87-f6d183af0b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabluary : ['books' 'coding' 'cricket' 'daily' 'he' 'learn' 'love' 'plays' 'python'\n",
            " 'reads' 'she' 'study' 'they' 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors: \")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2CkBgP0Hzqu",
        "outputId": "b3f59c69-5d33-42dd-94b1-7865f274eb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors: \n",
            "[[0 1 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 0 1]\n",
            " [0 0 1 0 1 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Term Frequency Inverse document frequency"
      ],
      "metadata": {
        "id": "qN3-u7GhH3Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words treates all words equally\n",
        "# for eg: I am learning pyhton, python and python\n",
        "#It will assign huge weight to python"
      ],
      "metadata": {
        "id": "BTO_6WxtH2LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "MO0FA1fYIL99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5a8ae4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer= TfidfVectorizer()\n",
        "sentences2=[ \"I like machine learning\",\n",
        "    \"I like deep learning\",\n",
        "    \"I like learning Python\",\n",
        "    \"machine learning is fun\",\n",
        "    \"deep learning uses neural networks\",\n",
        "    \"Python is used for machine learning\",\n",
        "    \"learning Python is fun\",\n",
        "    \"I like Python\"]\n",
        "X=vectorizer.fit_transform(sentences2)\n",
        "#internally it will\n",
        "# Build vocubalry\n",
        "# calculate TF\n",
        "# calculate idf\n",
        "# multiple tfxidf\n",
        "# genrerate matrix\n"
      ],
      "metadata": {
        "id": "8-gjjQ5ZIqFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocubalry: \", vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvUmgo9uIsBV",
        "outputId": "be41f7ae-8d22-4ced-bb20-92a7749bc6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocubalry:  ['deep' 'for' 'fun' 'is' 'learning' 'like' 'machine' 'networks' 'neural'\n",
            " 'python' 'used' 'uses']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF_ matrix:\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PPYU2--IsFJ",
        "outputId": "dc566433-b491-4277-c6dc-59cffd47c3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF_ matrix:\n",
            "[[0.         0.         0.         0.         0.42098223 0.59799617\n",
            "  0.68203705 0.         0.         0.         0.         0.        ]\n",
            " [0.73400135 0.         0.         0.         0.39095085 0.55533724\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.4456336  0.63301291\n",
            "  0.         0.         0.         0.63301291 0.         0.        ]\n",
            " [0.         0.         0.60052218 0.51820137 0.31985589 0.\n",
            "  0.51820137 0.         0.         0.         0.         0.        ]\n",
            " [0.42428841 0.         0.         0.         0.22598857 0.\n",
            "  0.         0.5062636  0.5062636  0.         0.         0.5062636 ]\n",
            " [0.         0.52361527 0.         0.37867468 0.23373409 0.\n",
            "  0.37867468 0.         0.         0.33201423 0.52361527 0.        ]\n",
            " [0.         0.         0.62008444 0.53508199 0.33027533 0.\n",
            "  0.         0.         0.         0.46914897 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.         0.         0.70710678 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#idf calculation:\n",
        "#log(total documents/ documents containtg word)"
      ],
      "metadata": {
        "id": "mRF3OSRxIsIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "b9_wlLuWIsL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9075033b-c886-43d8-e61d-a79d6e144750"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vansh-virmani/NextGenAI.git\n"
      ],
      "metadata": {
        "id": "_RL39iF4IsOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f9364ef-1865-4756-a233-df42a506010f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NextGenAI'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3AqtfCvIsRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGPeR8obIsVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}