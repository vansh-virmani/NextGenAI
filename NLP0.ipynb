{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeKlUNuq+/5Rg0WOYaKo7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vansh-virmani/NextGenAI/blob/main/NLP0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UlCDU795x2J",
        "outputId": "3cd5410e-3b52-4972-b37d-79c2e4bd97e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/nlp\")"
      ],
      "metadata": {
        "id": "pk2sst_6-Qmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"You currently have zero computing units available.\"\n",
        "text=text.lower()\n",
        "print(\"lower text: \",text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OD5uNmm69Eq",
        "outputId": "1d9b401d-571f-4cda-bdd9-7a333e1039d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lower text:  you currently have zero computing units available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lower the text"
      ],
      "metadata": {
        "id": "8xD1KMBT8J5c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHISiCwEhFfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step2 Remove Punctuation\n",
        "\n",
        "#we remove everything except letters and spaces\n",
        "# Using re-regular expression\n",
        "import re\n",
        "text=re.sub(r'[^\\w\\s]','',text)\n",
        "print(\"Without puncttuation: \",text)\n",
        "#[^\\w\\s]  to remove evrything except letters and spaces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTGuW4Li8UJ2",
        "outputId": "ed35716c-7ac0-4dd0-954c-a5c196ed7d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without puncttuation:  you currently have zero computing units available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step3 ***Tokenization***"
      ],
      "metadata": {
        "id": "cvNGunej9B54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split sentence into tokens\n",
        "tokens=text.split()\n",
        "print(\"tokens\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie6zltdB9A4K",
        "outputId": "2de2fe0e-3996-4bf4-dab1-5bfea56b07f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens ['you', 'currently', 'have', 'zero', 'computing', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step4 remove stopwords- is am are the a an very\n",
        "stopwords=[\"i\",\"am\",\"are\",\"have\",\"a\",\"an\",\"very\"]\n",
        "filtered_tokens=[]\n",
        "for word in tokens:\n",
        "  if word not in stopwords:\n",
        "    filtered_tokens.append(word)\n"
      ],
      "metadata": {
        "id": "52Ce19Bn9j4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"filtered tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ueEthw-bWn",
        "outputId": "4ae69bbf-0aeb-43b9-fa8d-67c2895ee91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filtered tokens: ['you', 'currently', 'zero', 'computing', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(word):\n",
        "  if word.endswith(\"ing\"):\n",
        "    return word[:-3]\n",
        "  return word\n",
        "stemmed_words=[]\n",
        "for word in filtered_tokens:\n",
        "  stemmed_words.append(stem(word))\n",
        "\n",
        "print(\"After stemming: \", stemmed_words)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sCNsE1e-mzC",
        "outputId": "c40202bc-b8f2-49db-c951-8f55814d8ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming:  ['you', 'currently', 'zero', 'comput', 'units', 'available']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP phase 2 word to vector\n",
        "#why ml- because ml only understands numbers text->numbrs vectorz\n",
        "from sklearn.feature_extraction.text import CountVectorizer #converting text to count\n",
        "sentences = [\n",
        "    \"I am learning Python every day\",\n",
        "    \"She likes to read books in the evening\",\n",
        "    \"We are building a small project together\",\n",
        "    \"He wants to become a data scientist\",\n",
        "    \"They are playing cricket in the park\",\n",
        "    \"I enjoy watching movies on weekends\",\n",
        "    \"The weather is very pleasant today\",\n",
        "    \"She is preparing for her exams seriously\",\n",
        "    \"We will start the project next week\",\n",
        "    \"He practices coding for two hours daily\",\n",
        "    \"The cat is sleeping on the sofa\",\n",
        "    \"I am trying to improve my communication skills\",\n",
        "    \"They are planning a trip to the mountains\",\n",
        "    \"She loves listening to music while studying\",\n",
        "    \"We are learning new technologies in college\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "JvJdBFgfAPQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer=CountVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)# learn the vocab and convert to numbers(count)\n",
        "print(\"Vocabluary :\", vectorizer.get_feature_names_out())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3XkN8FRBDyq",
        "outputId": "508ed4e6-ecba-4d68-9ca2-23935d0bf265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabluary : ['am' 'are' 'become' 'books' 'building' 'cat' 'coding' 'college'\n",
            " 'communication' 'cricket' 'daily' 'data' 'day' 'enjoy' 'evening' 'every'\n",
            " 'exams' 'for' 'he' 'her' 'hours' 'improve' 'in' 'is' 'learning' 'likes'\n",
            " 'listening' 'loves' 'mountains' 'movies' 'music' 'my' 'new' 'next' 'on'\n",
            " 'park' 'planning' 'playing' 'pleasant' 'practices' 'preparing' 'project'\n",
            " 'python' 'read' 'scientist' 'seriously' 'she' 'skills' 'sleeping' 'small'\n",
            " 'sofa' 'start' 'studying' 'technologies' 'the' 'they' 'to' 'today'\n",
            " 'together' 'trip' 'trying' 'two' 'very' 'wants' 'watching' 'we' 'weather'\n",
            " 'week' 'weekends' 'while' 'will']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors: \")\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u53CsxrFGf46",
        "outputId": "09a1e586-a579-43fa-d809-cc8bdb9ec5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors: \n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example-2\n",
        "# here each row= sentence\n",
        "# each column=word\n",
        "# each value=count\n",
        "sentences = [\n",
        "    \"I love coding\",\n",
        "    \"She reads books\",\n",
        "    \"We learn Python\",\n",
        "    \"He plays cricket\",\n",
        "    \"They study daily\"\n",
        "]\n",
        "vectorizer=CountVectorizer()\n",
        "X=vectorizer.fit_transform(sentences)# learn the vocab and convert to numbers\n",
        "print(\"Vocabluary :\", vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrWoUy8HGpwP",
        "outputId": "3d488082-cbe3-4e93-bd87-f6d183af0b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabluary : ['books' 'coding' 'cricket' 'daily' 'he' 'learn' 'love' 'plays' 'python'\n",
            " 'reads' 'she' 'study' 'they' 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vectors: \")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2CkBgP0Hzqu",
        "outputId": "b3f59c69-5d33-42dd-94b1-7865f274eb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectors: \n",
            "[[0 1 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 0 1]\n",
            " [0 0 1 0 1 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Term Frequency Inverse document frequency"
      ],
      "metadata": {
        "id": "qN3-u7GhH3Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words treates all words equally\n",
        "# for eg: I am learning pyhton, python and python\n",
        "#It will assign huge weight to python"
      ],
      "metadata": {
        "id": "BTO_6WxtH2LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "MO0FA1fYIL99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f5a8ae4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer= TfidfVectorizer()\n",
        "sentences2=[ \"I like machine learning\",\n",
        "    \"I like deep learning\",\n",
        "    \"I like learning Python\",\n",
        "    \"machine learning is fun\",\n",
        "    \"deep learning uses neural networks\",\n",
        "    \"Python is used for machine learning\",\n",
        "    \"learning Python is fun\",\n",
        "    \"I like Python\"]\n",
        "X=vectorizer.fit_transform(sentences2)\n",
        "#internally it will\n",
        "# Build vocubalry\n",
        "# calculate TF\n",
        "# calculate idf\n",
        "# multiple tfxidf\n",
        "# genrerate matrix\n"
      ],
      "metadata": {
        "id": "8-gjjQ5ZIqFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocubalry: \", vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvUmgo9uIsBV",
        "outputId": "be41f7ae-8d22-4ced-bb20-92a7749bc6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocubalry:  ['deep' 'for' 'fun' 'is' 'learning' 'like' 'machine' 'networks' 'neural'\n",
            " 'python' 'used' 'uses']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF_ matrix:\")\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PPYU2--IsFJ",
        "outputId": "dc566433-b491-4277-c6dc-59cffd47c3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF_ matrix:\n",
            "[[0.         0.         0.         0.         0.42098223 0.59799617\n",
            "  0.68203705 0.         0.         0.         0.         0.        ]\n",
            " [0.73400135 0.         0.         0.         0.39095085 0.55533724\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.4456336  0.63301291\n",
            "  0.         0.         0.         0.63301291 0.         0.        ]\n",
            " [0.         0.         0.60052218 0.51820137 0.31985589 0.\n",
            "  0.51820137 0.         0.         0.         0.         0.        ]\n",
            " [0.42428841 0.         0.         0.         0.22598857 0.\n",
            "  0.         0.5062636  0.5062636  0.         0.         0.5062636 ]\n",
            " [0.         0.52361527 0.         0.37867468 0.23373409 0.\n",
            "  0.37867468 0.         0.         0.33201423 0.52361527 0.        ]\n",
            " [0.         0.         0.62008444 0.53508199 0.33027533 0.\n",
            "  0.         0.         0.         0.46914897 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.         0.         0.70710678 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#idf calculation:\n",
        "#log(total documents/ documents containtg word)"
      ],
      "metadata": {
        "id": "mRF3OSRxIsIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "b9_wlLuWIsL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9075033b-c886-43d8-e61d-a79d6e144750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP0.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#4 major uses of ntlk\n",
        "# a.tokenization\n",
        "# b.stopword removal\n",
        "# c. stemming\n",
        "# d.Lemmaziation\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n"
      ],
      "metadata": {
        "id": "_RL39iF4IsOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3f1a4f-0f7b-4c24-e70a-10c91312bf81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "#For removing stopwords\n",
        "from nltk.corpus import stopwords\n",
        "#for stemming and lemmaziation\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "b3AqtfCvIsRb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I am learning Python programming, and it is very helpful!!!\"\n",
        "print(\"Orgiinal text:\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Tn2o3caipo",
        "outputId": "66a5dfbc-708e-4b16-c8e0-3b94790d9197"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orgiinal text: I am learning Python programming, and it is very helpful!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step-1:\n",
        "text=text.lower()\n",
        "print(\"After Lowercase: \", text)"
      ],
      "metadata": {
        "id": "KGPeR8obIsVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b8d51b-b4ac-4c71-988a-b354a7a73b80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lowercase:  i am learning python programming, and it is very helpful!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step-2\n",
        "#tokenization\n",
        "#we can use .split() but it is very complex for large datasets\n",
        "tokens=word_tokenize(text)\n",
        "print(\"tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVRJ0JznbGOw",
        "outputId": "30fdf9c3-a661-4401-bd13-1fa4bdac8402"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['i', 'am', 'learning', 'python', 'programming', ',', 'and', 'it', 'is', 'very', 'helpful', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuations\n",
        "import string\n",
        "punc=string.punctuation\n",
        "print(punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c08BQhHbntF",
        "outputId": "7d975308-6de1-416b-8868-38f7d4bb8f1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_filter=[word for word in tokens if word not in punc]\n",
        "punctuation_filter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC_ImJc0b1D_",
        "outputId": "34f1edb3-96eb-49bb-b2ed-c92e72e416b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'python',\n",
              " 'programming',\n",
              " 'and',\n",
              " 'it',\n",
              " 'is',\n",
              " 'very',\n",
              " 'helpful']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "eng_stopwords=stopwords.words(\"english\")\n"
      ],
      "metadata": {
        "id": "Y7bqb7h5cKrc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens=[word for word in punctuation_filter if word not in eng_stopwords]\n",
        "print(\"removed stopwords  we get sentence:\",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFwltC3Fcvai",
        "outputId": "0c03246d-6ad2-4096-cdd3-816b2fe2f6a7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "removed stopwords  we get sentence: ['learning', 'python', 'programming', 'helpful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 5 Stemming and Lemmatization\n",
        "stem= PorterStemmer()\n",
        "stem.stem(\"bought\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lWCgPGzPdxfl",
        "outputId": "98f949c9-0610-4fa5-829c-922fc5cb357a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bought'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnet=WordNetLemmatizer()\n",
        "print(wnet.lemmatize(\"playing\",\"v\"))\n",
        "print(wnet.lemmatize(\"flying\",\"v\")) #v ka matlab hai verb\n",
        "print(wnet.lemmatize(\"went\",\"v\"))\n",
        "print(wnet.lemmatize(\"bought\",\"v\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qs5UKJneKNr",
        "outputId": "7a40797c-4d21-415a-e4d9-962aea64c956"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "fly\n",
            "go\n",
            "buy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words=[]\n",
        "for word in filtered_tokens:\n",
        "  lemmatized_words.append(wnet.lemmatize(word,\"v\"))\n"
      ],
      "metadata": {
        "id": "gIKDws9beyvs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatization:\",lemmatized_words)\n",
        "cleaned_text=\" \".join(lemmatized_words)\n",
        "print(\"final text: \", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0JhCo59fJD0",
        "outputId": "7cbb86e8-37b0-4484-9c8e-77f70dbc328e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: ['learn', 'python', 'program', 'helpful']\n",
            "final text:  learn python program helpful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTyhdZSThIki",
        "outputId": "0f1ff8b0-bb84-4e9a-d19e-251993e3ed5a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}